# 1. KafkaProducerì˜ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜

## 1-1. ë ˆì½”ë“œ ì „ì†¡ í”„ë¡œì„¸ìŠ¤

### 1) KafkaProducer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

`Properties` ì¸ìŠ¤í„´ìŠ¤ì— key-value í˜•íƒœì˜ í™˜ê²½ì„¤ì • í•­ëª©ë“¤ì„ ì„¸íŒ…í•˜ê³  ì´ ì„¸íŒ… ë‚´ìš©ì„ `KafkaProducer` ìƒì„±ìì— ì „ë‹¬í•´ì„œ Producerë¥¼ ìƒì„±í•¨.

```java
// KafkaProducer í™˜ê²½ ì„¤ì •
Properties props = new Properties();
props.setProperty(BOOTSTRAP_SERVERS_CONFIG, "192.168.56.101:9092"); // Broker ì£¼ì†Œ (ì˜ˆì‹œëŠ” í•˜ë‚˜ì§€ë§Œ, ë³´í†µì€ ì—¬ëŸ¬ê°œ)
props.setProperty(KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // ë ˆì½”ë“œ key ì§ë ¬í™”ì— ì‚¬ìš©í•  Serializer í´ë˜ìŠ¤
props.setProperty(VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // ë ˆì½”ë“œ value ì§ë ¬í™”ì— ì‚¬ìš©í•  Serializer í´ë˜ìŠ¤

// KafkaProducer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
KafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(props);
```

ì´ ë•Œ `KafkaProducer`ì˜ ì œë„¤ë¦­ íƒ€ì…ì—ëŠ” ì¶”í›„ ì‚¬ìš©í•  ë ˆì½”ë“œì˜ key-value íƒ€ì…ì„ ì ìš©í•¨.
> ì˜ˆì‹œ ì½”ë“œì˜ ê²½ìš°ì—ëŠ” key(string)-value(string)

&nbsp;

ğŸ“Œ `KafkaProducer` ì¸ìŠ¤í„´ìŠ¤ê°€ ìƒì„±ë  ë•Œ, ì¶”í›„ ì‹¤ì œë¡œ ë ˆì½”ë“œë¥¼ ì „ì†¡í•˜ëŠ” ì—­í• ì„ í•  ë³„ë„ì˜ Sender ì“°ë ˆë“œê°€ ìƒì„±ë¨.

Sender ì“°ë ˆë“œëŠ” ë‚´ë¶€ ë ˆì½”ë“œ ë²„í¼ë¥¼ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ë©´ì„œ ë²„í¼ê°€ ì±„ì›Œì§€ë©´ Brokerë“¤ì—ê²Œ ì „ì†¡í•˜ëŠ” ì‘ì—…ì„ í•¨. (Batch ë°©ì‹)

> ì „ì†¡ ì‘ì—…ì€ ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì— ë©”ì¸ ì“°ë ˆë“œì˜ ì‹¤í–‰ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ.

&nbsp;

ğŸ“Œ `KafkaProducer` ì¸ìŠ¤í„´ìŠ¤ëŠ” thread-safeí•˜ê²Œ ì„¤ê³„ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ ì“°ë ˆë“œì—ì„œ í•œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê³µìœ í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

> ê·¸ëŸ¬ë‚˜ ì´ëŠ” ë©€í‹° ì“°ë ˆë“œ í™˜ê²½ì—ì„œ ë ˆì½”ë“œì˜ ì „ì†¡ ìˆœì„œ ë³´ì¥ì„ í•´ì£¼ì§€ëŠ” ì•Šìœ¼ë¯€ë¡œ, ìˆœì„œ ë³´ì¥ì´ í•„ìš”í•œ ê²½ìš°ì—ëŠ” ë³„ë„ì˜ ì¼€ì–´ê°€ í•„ìš”í•¨

&nbsp;

### 2) ProducerRecord ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

ìƒì„±í•´ë†“ì€ `KafkaProducer`ì˜ ì œë„¤ë¦­ íƒ€ì…ê³¼ ë™ì¼í•˜ê²Œ ë ˆì½”ë“œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•¨

ì´ ë•Œ `ProducerRecord`ì˜ ìƒì„±ìëŠ” ì—¬ëŸ¬ê°€ì§€ê°€ ìˆìœ¼ë‚˜, ìµœì†Œ êµ¬ì„± ìš”ì†ŒëŠ” Topic ì´ë¦„ê³¼ value ê°’ì„.

```java
String topic = "simple-topic";
ProducerRecord<String, String> producerRecord = new ProducerRecord<>(topic, "k1", "v1");
```

`ProducerRecord`ë¥¼ êµ¬ì„±í•˜ëŠ” í•„ë“œ

```java
public class ProducerRecord<K, V> {
    
    private final String topic;
    private final Integer partition;
    private final Headers headers;
    private final K key;
    private final V value;
    private final Long timestamp;
    
    ...
}
```

> ìƒì„± ë‹¨ê³„ì—ì„œ ëª…ì‹œë˜ì§€ ì•Šì€ êµ¬ì„± ìš”ì†ŒëŠ” ìë™ì ìœ¼ë¡œ `null`ë¡œ ë“¤ì–´ê°

&nbsp;

### 3) KafkaProducerì˜ send() ë©”ì„œë“œ í˜¸ì¶œ

ìƒì„±í•´ë†“ì€ `ProducerRecord` ì¸ìŠ¤í„´ìŠ¤ë¥¼ producerì˜ `send()` ë©”ì„œë“œì˜ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬

```java
kafkaProducer.send(producerRecord);
```

&nbsp;

## 1-2.ï¸ `send()` ë©”ì„œë“œì˜ ë‚´ë¶€ ë™ì‘

`send()` ë‚´ë¶€ì—ì„œ í˜¸ì¶œë˜ëŠ” `KafkaProducer`ì˜ `doSend()` ë©”ì„œë“œë¥¼ ëœ¯ì–´ë³´ë©´ ì•Œ ìˆ˜ ìˆìŒ

&nbsp;

### 1) Kafka Clusterë¡œë¶€í„° Topicì— ëŒ€í•œ ë©”íƒ€ ë°ì´í„° ìš”ì²­

ì‹¤ì œ ì „ì†¡ì— ì•ì„œ, ë ˆì½”ë“œë¥¼ ì „ì†¡í•  Topicì— ëŒ€í•œ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•¨ (`ProducerMetadata` ê°±ì‹ )

```java
try {
    clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);
}
```

&nbsp;

### 2) ì§ë ¬í™”(Serialization)

Configurationì— ë“±ë¡í•œ Serializerë¡œ keyì™€ valueë¥¼ ì§ë ¬í™”(byte arrayë¡œ ë³€í™˜)í•´ì„œ ì „ì†¡í•  ë ˆì½”ë“œì— ë‹´ìŒ

```java
byte[] serializedKey;
try {
    serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());
} catch (ClassCastException cce) {
    throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() +
            " to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +
            " specified in key.serializer", cce);
}
byte[] serializedValue;
try {
    serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());
} catch (ClassCastException cce) {
    throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() +
            " to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +
            " specified in value.serializer", cce);
}
```

&nbsp;

### 3) íŒŒí‹°ì…”ë‹(Partitioning)

ë ˆì½”ë“œì˜ key ìœ ë¬´ì— ë”°ë¼ ê°ê¸° ë‹¤ë¥¸ ì „ëµì„ ì‚¬ìš©í•´ ì–´ëŠ íŒŒí‹°ì…˜ì— ë ˆì½”ë“œë¥¼ ë³´ë‚¼ì§€ ì§€ì •í•¨.

```java
// Try to calculate partition, but note that after this call it can be RecordMetadata.UNKNOWN_PARTITION,
// which means that the RecordAccumulator would pick a partition using built-in logic (which may
// take into account broker load, the amount of data produced to each partition, etc.).
int partition = partition(record, serializedKey, serializedValue, cluster);
```

ë ˆì½”ë“œì— keyê°€ ì¡´ì¬í•˜ë©´ key í•´ì‹±ìœ¼ë¡œ íŒŒí‹°ì…”ë‹í•˜ê³  keyê°€ ì—†ìœ¼ë©´ ìš°ì„  íŒŒí‹°ì…˜ì„ `UNKNOWN_PARTITION`ìœ¼ë¡œ ì§€ì •í•´ë†“ê³  ì´í›„ì— `RecordAccumulator`ê°€ íŒŒí‹°ì…˜ì„ ì •í•˜ê²Œ í•¨.

```java
private int partition(ProducerRecord<K, V> record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) {
    if (record.partition() != null)
        return record.partition();

    if (partitioner != null) {
        int customPartition = partitioner.partition(
            record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);
        if (customPartition < 0) {
            throw new IllegalArgumentException(String.format(
                "The partitioner generated an invalid partition number: %d. Partition number should always be non-negative.", customPartition));
        }
        return customPartition;
    }

    if (serializedKey != null && !partitionerIgnoreKeys) {
        // hash the keyBytes to choose a partition
        return BuiltInPartitioner.partitionForKey(serializedKey, cluster.partitionsForTopic(record.topic()).size());
    } else {
        return RecordMetadata.UNKNOWN_PARTITION;
    }
}
```

ì•„ë˜ëŠ” `RecordAccumulator`ì˜ `append()` ì¤‘ ì¼ë¶€ë¡œ, `UNKNOWN_PARTITION`ì„ ê°€ì§„ ë ˆì½”ë“œì¸ ê²½ìš° ì§ì ‘ ë¸Œë¡œì»¤ ê°€ìš©ì„±ê³¼ ì„±ëŠ¥ì„ ê³ ë ¤í•´ Sticky Partitioningì„ í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ.

```java
// If the message doesn't have any partition affinity, so we pick a partition based on the broker
// availability and performance.  Note, that here we peek current partition before we hold the
// deque lock, so we'll need to make sure that it's not changed while we were waiting for the
// deque lock.
final BuiltInPartitioner.StickyPartitionInfo partitionInfo;
final int effectivePartition;
if (partition == RecordMetadata.UNKNOWN_PARTITION) {
    partitionInfo = topicInfo.builtInPartitioner.peekCurrentPartitionInfo(cluster);
    effectivePartition = partitionInfo.partition();
} else {
    partitionInfo = null;
    effectivePartition = partition;
}
```

> **ìš”ì•½**
>
> ğŸ“Œ keyê°€ ì—†ëŠ” ë ˆì½”ë“œì¸ ê²½ìš°:
>
> Sticky Partitioning ì „ëµìœ¼ë¡œ íŒŒí‹°ì…˜ ë„˜ë²„ë¥¼ ì§€ì •í•¨.
> 
> Sticky Partitioningì´ë€, ë°°ì¹˜ë¥¼ ìµœëŒ€í•œ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë°°ì¹˜ë¥¼ ê°€ë“ ì±„ìš°ê³  íŠ¹ì • íŒŒí‹°ì…˜ìœ¼ë¡œ ë³´ë‚´ ì±„ì›Œ ë‚˜ê°€ëŠ” ì „ëµ
> 
> <img src="https://cdn.confluent.io/wp-content/uploads/sticky-partitioner-strategy.png" height="480">
>
> &nbsp;
> 
> ğŸ“Œ keyë¥¼ ê°€ì§„ ë ˆì½”ë“œì¸ ê²½ìš°:
>
> ì§ë ¬í™”ëœ keyë¥¼ í•´ì‹±í•´ì„œ íŒŒí‹°ì…˜ ë„˜ë²„ë¥¼ ì§€ì •í•¨. ë™ì¼í•œ keyë¥¼ ê°€ì§„ë‹¤ë©´ ë™ì¼ íŒŒí‹°ì…˜ìœ¼ë¡œ ê°„ë‹¤ëŠ” ì˜ë¯¸.

&nbsp;

### 4) `RecordAccumulator`ì— ë ˆì½”ë“œ ì ì¬

`RecordAccumulator`ëŠ” ë¸Œë¡œì»¤ì— ë°°ì¹˜ ì „ì†¡ì„ í•˜ê¸° ìœ„í•´ ì „ì†¡í•  ë ˆì½”ë“œë“¤ì„ ëª¨ì•„ë‘ëŠ” ë²„í¼.

ë ˆì½”ë“œë¥¼ ê°œë³„ë¡œ ì „ì†¡í•˜ê²Œ ë˜ë©´ ë„¤íŠ¸ì›Œí¬ ìì› ë‚­ë¹„ê°€ ë°œìƒí•˜ê³ , ì „ì†¡ íš¨ìœ¨ì´ ë–¨ì–´ì§€ê¸° ë•Œë¬¸ì— ProducerëŠ” ë ˆì½”ë“œë¥¼ ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ ì „ì†¡í•¨

```java
RecordAccumulator.RecordAppendResult result = accumulator
        .append(record.topic(), partition, timestamp, serializedKey, serializedValue, headers, appendCallbacks, remainingWaitMs, abortOnNewBatch, nowMs, cluster);
```

ë ˆì½”ë“œë¥¼ ì ì¬í•˜ëŠ” ì—­í• ì„ í•˜ëŠ” `append()` ë©”ì„œë“œ

```java
public RecordAppendResult append(String topic, int partition, long timestamp, byte[] key,
                                     byte[] value, Header[] headers, AppendCallbacks callbacks, long maxTimeToBlock,
                                     boolean abortOnNewBatch, long nowMs, Cluster cluster) throws InterruptedException {
    TopicInfo topicInfo = topicInfoMap.computeIfAbsent(topic, k -> new TopicInfo(logContext, k, batchSize));
    
    try {
        // Loop to retry in case we encounter partitioner's race conditions.
        while (true) {
            ...
        
            Deque<ProducerBatch> dq = topicInfo.batches.computeIfAbsent(effectivePartition, k -> new ArrayDeque<>());
            synchronized (dq) {
                // After taking the lock, validate that the partition hasn't changed and retry.
                if (partitionChanged(topic, topicInfo, partitionInfo, dq, nowMs, cluster))
                    continue;

                RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callbacks, dq, nowMs);
                if (appendResult != null) {
                    // If queue has incomplete batches we disable switch (see comments in updatePartitionInfo).
                    boolean enableSwitch = allBatchesFull(dq);
                    topicInfo.builtInPartitioner.updatePartitionInfo(partitionInfo, appendResult.appendedBytes, cluster, enableSwitch);
                    return appendResult;
                }
            }

            ...
        }
    } finally {
        free.deallocate(buffer);
        appendsInProgress.decrementAndGet();
    }
}
```

ì—¬ê¸°ì„œ `TopicInfo`ëŠ” `RecordAccumulator`ì˜ ë‚´ë¶€ í´ë˜ìŠ¤ë¡œ, `ConcurrentMap` í˜•íƒœë¡œ ë ˆì½”ë“œ ë°°ì¹˜ë¥¼ ê°€ì§€ê³  ìˆìŒ.

> `ConcurrentMap`ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— thread-safeí•¨

```java
private static class TopicInfo {
    public final ConcurrentMap<Integer /*partition*/, Deque<ProducerBatch>> batches = new CopyOnWriteMap<>();
    public final BuiltInPartitioner builtInPartitioner;

    public TopicInfo(LogContext logContext, String topic, int stickyBatchSize) {
        builtInPartitioner = new BuiltInPartitioner(logContext, topic, stickyBatchSize);
    }
}
```

`batches`ëŠ” íŒŒí‹°ì…˜ IDë¥¼ keyë¡œ ê°–ê¸° ë•Œë¬¸ì— ë™ì¼í•œ í† í”½ê³¼ íŒŒí‹°ì…˜ IDë¥¼ ê°€ì§€ëŠ” ë ˆì½”ë“œë“¤ì€ ë™ì¼í•œ `ProducerBatch` ë°°ì¹˜ë¡œ ë¬¶ì—¬ì„œ ì „ì†¡ë¨.

&nbsp;

### 5) `Sender`ë¥¼ í†µí•œ ë ˆì½”ë“œ ì „ì†¡

`Sender`ëŠ” ì‹¤ì œ ë ˆì½”ë“œ ì „ì†¡ì„ ë‹´ë‹¹í•˜ëŠ” I/O ì“°ë ˆë“œë¡œ, `KafkaProducer`ê°€ ì´ˆê¸°í™”ë  ë•Œ í•¨ê»˜ ìƒì„±ë¨

```java
// KafkaProducer ìƒì„±ì ë‚´ë¶€ ì½”ë“œ
this.sender = newSender(logContext, kafkaClient, this.metadata);
String ioThreadName = NETWORK_THREAD_PREFIX + " | " + clientId;
this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
this.ioThread.start();
```

`doSend()` ë§ë¯¸ì— ë³´ë©´ ì „ì†¡í•  ë°°ì¹˜ê°€ ì¤€ë¹„ëì„ ë•Œ, ì´ `Sender` ì“°ë ˆë“œë¥¼ ê¹¨ì›Œ ì „ì†¡í•˜ê²Œë” í•¨

```java
if (result.batchIsFull || result.newBatchCreated) {
    log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), appendCallbacks.getPartition());
    this.sender.wakeup();
}
return result.future;
```

`Sender`ëŠ” `RecordAccumulator`ë¡œë¶€í„° ì „ì†¡ì„ ê¸°ë‹¤ë¦¬ëŠ” ë ˆì½”ë“œë“¤ì„ ê°€ì ¸ì™€ì„œ Brokerì—ê²Œ ë³´ëƒ„

```java
// Sender í´ë˜ìŠ¤ì˜ run() ë©”ì„œë“œ
@Override
public void run() {
    log.debug("Starting Kafka producer I/O thread.");

    // main loop, runs until close is called
    while (running) {
        try {
            runOnce();
        } catch (Exception e) {
            log.error("Uncaught error in kafka producer I/O thread: ", e);
        }
    }
    ...
}
```

`runOnce()` ë‚´ë¶€ì—ëŠ” ì „ì†¡í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•˜ëŠ” `sendProducerData()` í˜¸ì¶œë¶€ê°€ ìˆìŒ

```java
// run()ì´ í˜¸ì¶œí•œ runOnce() ë©”ì„œë“œ
void runOnce() {
    ...
    long currentTimeMs = time.milliseconds();
    long pollTimeout = sendProducerData(currentTimeMs);
    client.poll(pollTimeout, currentTimeMs);
}
```

`RecordAccumulator`ì˜ `drain()`ì„ í˜¸ì¶œí•´ ì „ì†¡í•  ë ˆì½”ë“œ ë°°ì¹˜ë¥¼ ê°€ì ¸ì˜¤ê³  `sendProduceRequests()`ë¥¼ í˜¸ì¶œí•´ ì „ì†¡í•¨

```java
// runOnce()ê°€ í˜¸ì¶œí•œ sendProducerData() ë©”ì„œë“œ
private long sendProducerData(long now) {
    Cluster cluster = metadata.fetch();
    // get the list of partitions with data ready to send
    RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);
    
    ...

    // create produce requests
    Map<Integer, List<ProducerBatch>> batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);
    addToInflightBatches(batches);

    ...
        
    sendProduceRequests(batches, now);
    return pollTimeout;
}
```