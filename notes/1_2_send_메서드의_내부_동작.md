# 1. KafkaProducer

## 1-2.ï¸ `send()` ë©”ì„œë“œì˜ ë‚´ë¶€ ë™ì‘

`send()`ëŠ” ë™ê¸° ë°©ì‹ì´ëƒ ë¹„ë™ê¸° ë°©ì‹ì´ëƒì— ë”°ë¼ ë‘ ì¢…ë¥˜ê°€ ì¡´ì¬í•¨

1. `send(ProducerRecord<K, V> record)`:

- ë™ê¸° ë°©ì‹

- ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ë©´, Brokerë¡œë¶€í„° ack ì‘ë‹µì„ ë°›ì„ ë•Œê¹Œì§€ block ìƒíƒœ

2. `send(ProducerRecord<K, V> record, Callback callback)`

- ë¹„ë™ê¸° ë°©ì‹

- ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ë©´, Brokerë¡œë¶€í„° ack ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ë‹¤ìŒ ì‘ì—…ì„ ê³„ì†í•¨ (non-block)

- ë‚˜ì¤‘ì— ack ì‘ë‹µì„ ë°›ìœ¼ë©´, ê·¸ ë•Œ Callbackì´ í˜¸ì¶œë˜ì–´ ë™ì‘í•¨ (metadataì™€ exception í•¸ë“¤ë§)

 
1ë²ˆ `send()`ëŠ” ë‚´ë¶€ì—ì„œ Callbackì„ `null`ë¡œ ë‘ê³  2ë²ˆ `send()`ë¥¼ í˜¸ì¶œí•¨

ë”°ë¼ì„œ ë©”ì‹œì§€ ì „ì†¡ í”„ë¡œì„¸ìŠ¤ ìì²´ëŠ” 2ë²ˆ `send()` ë‚´ë¶€ì—ì„œ í˜¸ì¶œë˜ëŠ” `doSend()`ë¼ëŠ” ë©”ì„œë“œë¥¼ ëœ¯ì–´ë³´ë©´ ì•Œ ìˆ˜ ìˆìŒ

&nbsp;

### 1) Kafka Clusterë¡œë¶€í„° Topicì— ëŒ€í•œ ë©”íƒ€ ë°ì´í„° ìš”ì²­

ì‹¤ì œ ì „ì†¡ì— ì•ì„œ, ë ˆì½”ë“œë¥¼ ì „ì†¡í•  Topicì— ëŒ€í•œ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•¨ (`ProducerMetadata` ê°±ì‹ )

```java
try {
    clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);
}
```

&nbsp;

### 2) ì§ë ¬í™”(Serialization)

Configurationì— ë“±ë¡í•œ Serializerë¡œ keyì™€ valueë¥¼ ì§ë ¬í™”(byte arrayë¡œ ë³€í™˜)í•´ì„œ ì „ì†¡í•  ë ˆì½”ë“œì— ë‹´ìŒ

```java
byte[] serializedKey;
try {
    serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());
} catch (ClassCastException cce) {
    throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() +
            " to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +
            " specified in key.serializer", cce);
}
byte[] serializedValue;
try {
    serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());
} catch (ClassCastException cce) {
    throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() +
            " to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +
            " specified in value.serializer", cce);
}
```

&nbsp;

### 3) íŒŒí‹°ì…”ë‹(Partitioning)

ë ˆì½”ë“œì˜ key ìœ ë¬´ì— ë”°ë¼ ê°ê¸° ë‹¤ë¥¸ ì „ëµì„ ì‚¬ìš©í•´ ì–´ëŠ íŒŒí‹°ì…˜ì— ë ˆì½”ë“œë¥¼ ë³´ë‚¼ì§€ ì§€ì •í•¨.

```java
// Try to calculate partition, but note that after this call it can be RecordMetadata.UNKNOWN_PARTITION,
// which means that the RecordAccumulator would pick a partition using built-in logic (which may
// take into account broker load, the amount of data produced to each partition, etc.).
int partition = partition(record, serializedKey, serializedValue, cluster);
```

ë ˆì½”ë“œì— keyê°€ ì¡´ì¬í•˜ë©´ key í•´ì‹±ìœ¼ë¡œ íŒŒí‹°ì…”ë‹í•˜ê³  keyê°€ ì—†ìœ¼ë©´ ìš°ì„  íŒŒí‹°ì…˜ì„ `UNKNOWN_PARTITION`ìœ¼ë¡œ ì§€ì •í•´ë†“ê³  ì´í›„ì— `RecordAccumulator`ê°€ íŒŒí‹°ì…˜ì„ ì •í•˜ê²Œ í•¨.

```java
private int partition(ProducerRecord<K, V> record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) {
    if (record.partition() != null)
        return record.partition();

    if (partitioner != null) {
        int customPartition = partitioner.partition(
            record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);
        if (customPartition < 0) {
            throw new IllegalArgumentException(String.format(
                "The partitioner generated an invalid partition number: %d. Partition number should always be non-negative.", customPartition));
        }
        return customPartition;
    }

    if (serializedKey != null && !partitionerIgnoreKeys) {
        // hash the keyBytes to choose a partition
        return BuiltInPartitioner.partitionForKey(serializedKey, cluster.partitionsForTopic(record.topic()).size());
    } else {
        return RecordMetadata.UNKNOWN_PARTITION;
    }
}
```

ì•„ë˜ëŠ” `RecordAccumulator`ì˜ `append()` ì¤‘ ì¼ë¶€ë¡œ, `UNKNOWN_PARTITION`ì„ ê°€ì§„ ë ˆì½”ë“œì¸ ê²½ìš° ì§ì ‘ ë¸Œë¡œì»¤ ê°€ìš©ì„±ê³¼ ì„±ëŠ¥ì„ ê³ ë ¤í•´ Sticky Partitioningì„ í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ.

```java
// If the message doesn't have any partition affinity, so we pick a partition based on the broker
// availability and performance.  Note, that here we peek current partition before we hold the
// deque lock, so we'll need to make sure that it's not changed while we were waiting for the
// deque lock.
final BuiltInPartitioner.StickyPartitionInfo partitionInfo;
final int effectivePartition;
if (partition == RecordMetadata.UNKNOWN_PARTITION) {
    partitionInfo = topicInfo.builtInPartitioner.peekCurrentPartitionInfo(cluster);
    effectivePartition = partitionInfo.partition();
} else {
    partitionInfo = null;
    effectivePartition = partition;
}
```

> **ìš”ì•½**
>
> ğŸ“Œ keyê°€ ì—†ëŠ” ë ˆì½”ë“œì¸ ê²½ìš°:
>
> Sticky Partitioning ì „ëµìœ¼ë¡œ íŒŒí‹°ì…˜ ë„˜ë²„ë¥¼ ì§€ì •í•¨.
> 
> Sticky Partitioningì´ë€, ë°°ì¹˜ë¥¼ ìµœëŒ€í•œ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë°°ì¹˜ë¥¼ ê°€ë“ ì±„ìš°ê³  íŠ¹ì • íŒŒí‹°ì…˜ìœ¼ë¡œ ë³´ë‚´ ì±„ì›Œ ë‚˜ê°€ëŠ” ì „ëµ
> 
> <img src="https://cdn.confluent.io/wp-content/uploads/sticky-partitioner-strategy.png" height="480">
>
> &nbsp;
> 
> ğŸ“Œ keyë¥¼ ê°€ì§„ ë ˆì½”ë“œì¸ ê²½ìš°:
>
> ì§ë ¬í™”ëœ keyë¥¼ í•´ì‹±í•´ì„œ íŒŒí‹°ì…˜ ë„˜ë²„ë¥¼ ì§€ì •í•¨. ë™ì¼í•œ keyë¥¼ ê°€ì§„ë‹¤ë©´ ë™ì¼ íŒŒí‹°ì…˜ìœ¼ë¡œ ê°„ë‹¤ëŠ” ì˜ë¯¸.

&nbsp;

### 4) `RecordAccumulator`ì— ë ˆì½”ë“œ ì ì¬

`RecordAccumulator`ëŠ” ë¸Œë¡œì»¤ì— ë°°ì¹˜ ì „ì†¡ì„ í•˜ê¸° ìœ„í•´ ì „ì†¡í•  ë ˆì½”ë“œë“¤ì„ ëª¨ì•„ë‘ëŠ” ë²„í¼.

ë ˆì½”ë“œë¥¼ ê°œë³„ë¡œ ì „ì†¡í•˜ê²Œ ë˜ë©´ ë„¤íŠ¸ì›Œí¬ ìì› ë‚­ë¹„ê°€ ë°œìƒí•˜ê³ , ì „ì†¡ íš¨ìœ¨ì´ ë–¨ì–´ì§€ê¸° ë•Œë¬¸ì— ProducerëŠ” ë ˆì½”ë“œë¥¼ ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ ì „ì†¡í•¨

```java
RecordAccumulator.RecordAppendResult result = accumulator
        .append(record.topic(), partition, timestamp, serializedKey, serializedValue, headers, appendCallbacks, remainingWaitMs, abortOnNewBatch, nowMs, cluster);
```

ë ˆì½”ë“œë¥¼ ì ì¬í•˜ëŠ” ì—­í• ì„ í•˜ëŠ” `append()` ë©”ì„œë“œ

```java
public RecordAppendResult append(String topic, int partition, long timestamp, byte[] key,
                                     byte[] value, Header[] headers, AppendCallbacks callbacks, long maxTimeToBlock,
                                     boolean abortOnNewBatch, long nowMs, Cluster cluster) throws InterruptedException {
    TopicInfo topicInfo = topicInfoMap.computeIfAbsent(topic, k -> new TopicInfo(logContext, k, batchSize));
    
    try {
        // Loop to retry in case we encounter partitioner's race conditions.
        while (true) {
            ...
        
            Deque<ProducerBatch> dq = topicInfo.batches.computeIfAbsent(effectivePartition, k -> new ArrayDeque<>());
            synchronized (dq) {
                // After taking the lock, validate that the partition hasn't changed and retry.
                if (partitionChanged(topic, topicInfo, partitionInfo, dq, nowMs, cluster))
                    continue;

                RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callbacks, dq, nowMs);
                if (appendResult != null) {
                    // If queue has incomplete batches we disable switch (see comments in updatePartitionInfo).
                    boolean enableSwitch = allBatchesFull(dq);
                    topicInfo.builtInPartitioner.updatePartitionInfo(partitionInfo, appendResult.appendedBytes, cluster, enableSwitch);
                    return appendResult;
                }
            }

            ...
        }
    } finally {
        free.deallocate(buffer);
        appendsInProgress.decrementAndGet();
    }
}
```

ì—¬ê¸°ì„œ `TopicInfo`ëŠ” `RecordAccumulator`ì˜ ë‚´ë¶€ í´ë˜ìŠ¤ë¡œ, `ConcurrentMap` í˜•íƒœë¡œ ë ˆì½”ë“œ ë°°ì¹˜ë¥¼ ê°€ì§€ê³  ìˆìŒ.

> `ConcurrentMap`ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— thread-safeí•¨

```java
private static class TopicInfo {
    public final ConcurrentMap<Integer /*partition*/, Deque<ProducerBatch>> batches = new CopyOnWriteMap<>();
    public final BuiltInPartitioner builtInPartitioner;

    public TopicInfo(LogContext logContext, String topic, int stickyBatchSize) {
        builtInPartitioner = new BuiltInPartitioner(logContext, topic, stickyBatchSize);
    }
}
```

`batches`ëŠ” íŒŒí‹°ì…˜ IDë¥¼ keyë¡œ ê°–ê¸° ë•Œë¬¸ì— ë™ì¼í•œ í† í”½ê³¼ íŒŒí‹°ì…˜ IDë¥¼ ê°€ì§€ëŠ” ë ˆì½”ë“œë“¤ì€ ë™ì¼í•œ `ProducerBatch` ë°°ì¹˜ë¡œ ë¬¶ì—¬ì„œ ì „ì†¡ë¨.

&nbsp;

### 5) `Sender`ë¥¼ í†µí•œ ë ˆì½”ë“œ ì „ì†¡

`Sender`ëŠ” ì‹¤ì œ ë ˆì½”ë“œ ì „ì†¡ì„ ë‹´ë‹¹í•˜ëŠ” I/O ì“°ë ˆë“œë¡œ, `KafkaProducer`ê°€ ì´ˆê¸°í™”ë  ë•Œ í•¨ê»˜ ìƒì„±ë¨

```java
// KafkaProducer ìƒì„±ì ë‚´ë¶€ ì½”ë“œ
this.sender = newSender(logContext, kafkaClient, this.metadata);
String ioThreadName = NETWORK_THREAD_PREFIX + " | " + clientId;
this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
this.ioThread.start();
```

`doSend()` ë§ë¯¸ì— ë³´ë©´ ì „ì†¡í•  ë°°ì¹˜ê°€ ì¤€ë¹„ëì„ ë•Œ, ì´ `Sender` ì“°ë ˆë“œë¥¼ ê¹¨ì›Œ ì „ì†¡í•˜ê²Œë” í•¨

```java
if (result.batchIsFull || result.newBatchCreated) {
    log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), appendCallbacks.getPartition());
    this.sender.wakeup();
}
return result.future;
```

`Sender`ëŠ” `RecordAccumulator`ë¡œë¶€í„° ì „ì†¡ì„ ê¸°ë‹¤ë¦¬ëŠ” ë ˆì½”ë“œë“¤ì„ ê°€ì ¸ì™€ì„œ Brokerì—ê²Œ ë³´ëƒ„

```java
// Sender í´ë˜ìŠ¤ì˜ run() ë©”ì„œë“œ
@Override
public void run() {
    log.debug("Starting Kafka producer I/O thread.");

    // main loop, runs until close is called
    while (running) {
        try {
            runOnce();
        } catch (Exception e) {
            log.error("Uncaught error in kafka producer I/O thread: ", e);
        }
    }
    ...
}
```

`runOnce()` ë‚´ë¶€ì—ëŠ” ì „ì†¡í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•˜ëŠ” `sendProducerData()` í˜¸ì¶œë¶€ê°€ ìˆìŒ

```java
// run()ì´ í˜¸ì¶œí•œ runOnce() ë©”ì„œë“œ
void runOnce() {
    ...
    long currentTimeMs = time.milliseconds();
    long pollTimeout = sendProducerData(currentTimeMs);
    client.poll(pollTimeout, currentTimeMs);
}
```

`RecordAccumulator`ì˜ `drain()`ì„ í˜¸ì¶œí•´ ì „ì†¡í•  ë ˆì½”ë“œ ë°°ì¹˜ë¥¼ ê°€ì ¸ì˜¤ê³  `sendProduceRequests()`ë¥¼ í˜¸ì¶œí•´ ì „ì†¡í•¨

```java
// runOnce()ê°€ í˜¸ì¶œí•œ sendProducerData() ë©”ì„œë“œ
private long sendProducerData(long now) {
    Cluster cluster = metadata.fetch();
    // get the list of partitions with data ready to send
    RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);
    
    ...

    // create produce requests
    Map<Integer, List<ProducerBatch>> batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);
    addToInflightBatches(batches);

    ...
        
    sendProduceRequests(batches, now);
    return pollTimeout;
}
```